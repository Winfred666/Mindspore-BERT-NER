{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking /home/winfred/.mscache/mindspore/1.9/bertfinetune_bilstmcrf_chinesener.md...\u001b[1;32mPassed!\u001b[0m\n",
      "Warning. Can't find net cache, will reloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n",
      "hint: is subject to change. To configure the initial branch name to use in all\n",
      "hint: of your new repositories, which will suppress this warning, call:\n",
      "hint: \n",
      "hint: \tgit config --global init.defaultBranch <name>\n",
      "hint: \n",
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
      "hint: 'development'. The just-created branch can be renamed via this command:\n",
      "hint: \n",
      "hint: \tgit branch -m <name>\n",
      "From https://gitee.com/mindspore/models\n",
      " * [new branch]          dynamic_shape -> origin/dynamic_shape\n",
      " * [new branch]          master        -> origin/master\n",
      " * [new branch]          r1.10         -> origin/r1.10\n",
      " * [new branch]          r1.2          -> origin/r1.2\n",
      " * [new branch]          r1.3          -> origin/r1.3\n",
      " * [new branch]          r1.4          -> origin/r1.4\n",
      " * [new branch]          r1.5          -> origin/r1.5\n",
      " * [new branch]          r1.6          -> origin/r1.6\n",
      " * [new branch]          r1.7          -> origin/r1.7\n",
      " * [new branch]          r1.8          -> origin/r1.8\n",
      " * [new branch]          r1.9          -> origin/r1.9\n",
      " * [new branch]          r2.0          -> origin/r2.0\n",
      " * [new branch]          r2.0.0-alpha  -> origin/r2.0.0-alpha\n",
      " * [new branch]          r2.1          -> origin/r2.1\n",
      " * [new branch]          r2.2          -> origin/r2.2\n",
      " * [new branch]          r2.3          -> origin/r2.3\n",
      " * [new tag]             v1.5.0        -> v1.5.0\n",
      " * [new tag]             v1.6.0        -> v1.6.0\n",
      " * [new tag]             v2.2.1        -> v2.2.1\n",
      " * [new tag]             v2.2.10       -> v2.2.10\n",
      " * [new tag]             v2.2.11       -> v2.2.11\n",
      "From https://gitee.com/mindspore/models\n",
      " * branch                r1.9       -> FETCH_HEAD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking /home/winfred/.mscache/mindspore/1.9/bertfinetune_bilstmcrf_chinesener.md...\u001b[1;32mPassed!\u001b[0m\n",
      "Warning. The `bertfinetune_bilstmcrf_ascend_v190_chinesener_official_nlp_F1score96.07.ckpt` is not exist in local, it will auto-download.\n",
      "Downloading data from url https://download.mindspore.cn/models/r1.9/bertfinetune_bilstmcrf_ascend_v190_chinesener_official_nlp_F1score96.07.ckpt\n",
      "Downloading...100.0%\n",
      "Download finished!\n",
      "File size = 1.18 Gb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(235047:140422335078400,MainProcess):2024-09-10-12:19:24.740.871 [mindspore/train/serialization.py:736] For 'load_param_into_net', remove parameter prefix name: bert.bert., continue to load.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel<\n",
       "  (bert_embedding_lookup): Embedding<vocab_size=21128, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=bert_embedding_lookup.embedding_table, shape=(21128, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (bert_embedding_postprocessor): EmbeddingPostprocessor<\n",
       "    (token_type_embedding): Embedding<vocab_size=2, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=bert_embedding_postprocessor.token_type_embedding.embedding_table, shape=(2, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (dropout): Dropout<keep_prob=1.0>\n",
       "    (full_position_embedding): Embedding<vocab_size=512, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=bert_embedding_postprocessor.full_position_embedding.embedding_table, shape=(512, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "    (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_embedding_postprocessor.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_embedding_postprocessor.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "    >\n",
       "  (bert_encoder): BertTransformer<\n",
       "    (layers): CellList<\n",
       "      (0): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.0.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.0.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.0.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.0.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (1): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.1.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.1.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.1.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.1.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (2): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.2.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.2.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.2.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.2.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (3): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.3.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.3.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.3.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.3.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (4): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.4.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.4.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.4.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.4.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (5): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.5.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.5.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.5.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.5.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (6): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.6.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.6.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.6.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.6.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (7): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.7.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.7.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.7.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.7.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (8): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.8.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.8.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.8.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.8.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (9): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.9.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.9.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.9.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.9.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (10): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.10.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.10.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.10.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.10.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      (11): BertEncoderCell<\n",
       "        (attention): BertSelfAttention<\n",
       "          (attention): BertAttention<\n",
       "            (query_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (key_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (value_layer): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (softmax): Softmax<>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (cast_compute_type): SaturateCast<>\n",
       "            >\n",
       "          (output): BertOutput<\n",
       "            (dense): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "            (dropout): Dropout<keep_prob=1.0>\n",
       "            (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.11.attention.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.11.attention.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "            >\n",
       "          >\n",
       "        (intermediate): Dense<\n",
       "          input_channels=768, output_channels=3072, has_bias=True, activation=GELU<>\n",
       "          (activation): GELU<>\n",
       "          >\n",
       "        (output): BertOutput<\n",
       "          (dense): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          (layernorm): LayerNorm<normalized_shape=(768,), begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=bert_encoder.layers.11.output.layernorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=bert_encoder.layers.11.output.layernorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "          >\n",
       "        >\n",
       "      >\n",
       "    >\n",
       "  (cast_compute_type): SaturateCast<>\n",
       "  (dense): Dense<\n",
       "    input_channels=768, output_channels=768, has_bias=True, activation=Tanh<>\n",
       "    (activation): Tanh<>\n",
       "    >\n",
       "  (_create_attention_mask_from_input_mask): CreateAttentionMaskFromInputMask<>\n",
       "  >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore_hub as mshub\n",
    "\n",
    "model = \"mindspore/1.9/bertfinetune_bilstmcrf_chinesener\"\n",
    "# the model will be downloaded at /home/winfred/.mscache/mindspore/1.9/\n",
    "network = mshub.load(model)\n",
    "network.set_train(False)\n",
    "# then need to change ckpt path:\n",
    "# ! vim /home/winfred/.mscache/mindspore/1.9/bertfinetune_bilstmcrf_chinesener.md\n",
    "# ! ChineseNER -> chineseNer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
